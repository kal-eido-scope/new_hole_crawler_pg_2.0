{"code": 0, "data": {"allow_search": true, "attention": false, "author_title": null, "blocked": false, "blocked_count": null, "can_del": false, "comments": [{"author_title": "", "blocked": false, "blocked_count": null, "can_del": false, "cid": 179269, "create_time": 1654086978, "is_blocked": false, "is_tmp": false, "name_id": 1, "text": "我感觉问题在于别的bug", "timestamp": 1654086978}], "create_time": 1654086880, "cw": null, "hot_score": null, "is_blocked": false, "is_reported": null, "is_tmp": false, "last_comment_time": 1654086978, "likenum": 2, "n_attentions": 2, "n_comments": 1, "pid": 56846, "poll": null, "reply": 1, "text": "RNN 梯度消失怎么办\n用了 ReLU 和正交初始化\n但问题在于，如果使用正交矩阵，那 ReLU 在每一层砍掉一半的信息，那还能保证每层权值分布相同吗", "timestamp": 1654086880}}