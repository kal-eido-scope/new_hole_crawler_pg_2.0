{"code": 0, "data": {"allow_search": true, "attention": false, "author_title": null, "blocked": false, "blocked_count": null, "can_del": false, "comments": [{"author_title": "", "blocked": false, "blocked_count": null, "can_del": false, "cid": 87098, "create_time": 1650906444, "is_blocked": false, "is_tmp": false, "name_id": 0, "text": "model(input)为啥可以这样写 #31403 感觉这个问题有点没意思了。之前倒是也好奇过这个问题，但忙着写代码，没去仔细查一下。", "timestamp": 1650906444}, {"author_title": "", "blocked": false, "blocked_count": null, "can_del": false, "cid": 87101, "create_time": 1650906638, "is_blocked": false, "is_tmp": false, "name_id": 0, "text": "model.train()和model.eval()我回答的是训练模式会把dropout层打开，eval模式就不用dropout了。他问dropout具体是怎么实现的，我说就是一层中随机抛弃几个节点，剩下的节点的数值按照比例变化。eval的时候就不用抛弃了。这个我感觉答的应该没问题", "timestamp": 1650906638}, {"author_title": "", "blocked": false, "blocked_count": null, "can_del": false, "cid": 87103, "create_time": 1650906819, "is_blocked": false, "is_tmp": false, "name_id": 0, "text": "优化器这个倒是从学完之后就没复习过了，也没明白他想问啥。我当时做多任务的时候，理所当然的就觉得把一个任务的loss增大，模型就会优先优化这个任务。他问为什么，我说既然该任务loss大，那梯度就大，更新的幅度大，所以模型就朝着有利于该任务的方向更新😅很明显他不满意", "timestamp": 1650906819}, {"author_title": "", "blocked": false, "blocked_count": null, "can_del": false, "cid": 87107, "create_time": 1650907164, "is_blocked": false, "is_tmp": false, "name_id": 0, "text": "第一道算法题算auc，他先是教了我这个计算方法，任取一对正负例，然后看他们的预测值是不是正的比负的大，记录一下有多少对是顺序的，然后除以正例个数*负例个数。电话沟通了半天我才听懂这个算法。然后他问我这个n2的复杂度怎么优化，我给了一个nlogn的方法，但是百度了一下发现有更简单的，虽然也是nlogn。这道题怪我不会算AUC，之前看的书没写这个计算方法，导致我一直认为算AUC就是算ROC曲线下的面积。百度了一下发现都是用他说的这个算法算的", "timestamp": 1650907164}, {"author_title": "", "blocked": false, "blocked_count": null, "can_del": false, "cid": 87108, "create_time": 1650907327, "is_blocked": false, "is_tmp": false, "name_id": 0, "text": "二面算法题我是先反转链表，然后for循环一直加，加完再反转回去的。反转链表没背下来，现场写了很久，而且写的没有标准写法那么简洁。不过好歹写出来了", "timestamp": 1650907327}, {"author_title": "", "blocked": false, "blocked_count": null, "can_del": false, "cid": 87110, "create_time": 1650907527, "is_blocked": false, "is_tmp": false, "name_id": 0, "text": "可能是因为没做过推荐系统的项目，导致二面面试官没得问，然后只能问些刁钻的问题吧。反正给我的面试体验也极差，经常打断我发言，我讲完项目他说听不懂，然后自己问别的内容", "timestamp": 1650907527}, {"author_title": "", "blocked": false, "blocked_count": null, "can_del": false, "cid": 87130, "create_time": 1650909060, "is_blocked": false, "is_tmp": false, "name_id": 1, "text": "dz是硕士还是博士呀？面的暑期实习还是全职工作？", "timestamp": 1650909060}, {"author_title": "", "blocked": false, "blocked_count": null, "can_del": false, "cid": 87132, "create_time": 1650909224, "is_blocked": false, "is_tmp": false, "name_id": 0, "text": "Re Alice: 硕士 暑期", "timestamp": 1650909224}, {"author_title": "", "blocked": false, "blocked_count": null, "can_del": false, "cid": 87175, "create_time": 1650935157, "is_blocked": false, "is_tmp": false, "name_id": 2, "text": "modeltrain和eval应该考虑到bn层，loss乘系数应该考虑到weight decay的sgd和adam的不同，这几点还挺重要的，dz再复习一下，感谢分享", "timestamp": 1650935157}, {"author_title": "", "blocked": false, "blocked_count": null, "can_del": false, "cid": 87265, "create_time": 1650944095, "is_blocked": false, "is_tmp": false, "name_id": 3, "text": "感谢分享！同在面暑期实习", "timestamp": 1650944095}], "create_time": 1650906263, "cw": null, "hot_score": null, "is_blocked": false, "is_reported": null, "is_tmp": false, "last_comment_time": 1650944095, "likenum": 50, "n_attentions": 50, "n_comments": 10, "pid": 31834, "poll": null, "reply": 10, "text": "记录一下微软面试是怎么挂掉的，面必应广告部门。一面二面一起。一面问的自身项目相关的东西，一直在聊项目。算法题考的是计算AUC，按照AUC的概率意义计算，任取一对正例和负例，正例的预测值大于负例的预测值的概率。二面的面试官对我的项目不感兴趣，就一直在问一些细节知识。让我把pytorch训练代码那几行写出来，就是model(input)，zero_grad()，backward()，step()那几行。然后问为什么model(input)可以这样写。model.eval()和model.train()代表啥。还问了transformer具体结构，用的是layernorm还是batchnorm，残差加在norm之前还是之后，有几个残差。我只回答对了用的layernorm，剩下的都不太记得了，他就没有继续问。还问了优化器adam sgd在loss乘一个系数之后更新有什么变化(这个是因为我提到了多任务训练中把一个任务的loss乘一个系数，希望这个任务在训练中占优势)。还问我做的项目最后网络输出的label有几类，最后的loss是什么数量级的(实在没搞懂为啥问这个，回答完他就过了)。二面算法题是把两个用链表储存的整数相加，最后返回一个链表。", "timestamp": 1650906263}}